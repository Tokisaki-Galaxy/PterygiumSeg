{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "view-in-github",
    "papermill": {
     "duration": 0.00543,
     "end_time": "2025-04-13T01:53:45.044748",
     "exception": false,
     "start_time": "2025-04-13T01:53:45.039318",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Tokisaki-Galaxy/PterygiumSeg/blob/master/work1_basemodel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "id": "I8rV7E66jSoG",
    "papermill": {
     "duration": 0.004352,
     "end_time": "2025-04-13T01:53:45.053747",
     "exception": false,
     "start_time": "2025-04-13T01:53:45.049395",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 导入必要的库\n",
    "导入PyTorch、OpenCV、Pandas等必要的库，为图像分类模型做准备。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "id": "e-siDBWjjSo6",
    "papermill": {
     "duration": 11.426914,
     "end_time": "2025-04-13T01:53:56.485296",
     "exception": false,
     "start_time": "2025-04-13T01:53:45.058382",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torchvision import transforms, models\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import platform\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "from tqdm.autonotebook import tqdm\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager\n",
    "\n",
    "if platform.system() == \"Windows\":\n",
    "    num_workers = 0\n",
    "    print(f\"检测到 Windows 系统，将 DataLoader 的 num_workers 设置为 {num_workers}。\")\n",
    "else:\n",
    "    # 在非 Windows 系统（如 Linux/Colab）上\n",
    "    num_workers = 4\n",
    "    print(f\"检测到非 Windows 系统 ({platform.system()})，将 DataLoader 的 num_workers 设置为 {num_workers}。\")\n",
    "    # 设置中文字体\n",
    "    if not os.path.exists('simhei.ttf'):\n",
    "        !wget -O simhei.ttf \"https://cdn.jsdelivr.net/gh/Haixing-Hu/latex-chinese-fonts/chinese/%E9%BB%91%E4%BD%93/SimHei.ttf\"\n",
    "    matplotlib.font_manager.fontManager.addfont('simhei.ttf')\n",
    "    matplotlib.rc('font', family='SimHei')\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# ================== 数据集路径 =================\n",
    "# 数据路径\n",
    "image_dir =          r\"f:/train\"\n",
    "# colab路径\n",
    "colab_zip_path = \"/content/drive/My Drive/train.zip\"\n",
    "colab_extract_path = \"/content/trains/\"\n",
    "# Kaggle路径\n",
    "#kaggle_zip_path = \"/kaggle/working/train.zip\"\n",
    "#kaggle_extract_path = \"/kaggle/working/trains/\"\n",
    "kaggle_extract_path = \"/kaggle/input/pterygium/train/\"\n",
    "kaggle_temp_path = \"/kaggle/working/\"\n",
    "\n",
    "# =================== 验证集路径 =================\n",
    "# 验证集路径\n",
    "val_image_dir =      r\"f:/val\"\n",
    "# colab路径\n",
    "#colab_val_zip_path = \"/content/drive/My Drive/val.zip\"\n",
    "#colab_val_extract_path = \"/content/val/\"\n",
    "# Kaggle路径\n",
    "kaggle_val_path = \"/kaggle/input/pterygium/val_img/\"\n",
    "\n",
    "# 配置GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"CUDA 可用: {torch.cuda.is_available()}\")\n",
    "print(f\"使用的设备: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    cudnn.benchmark = True\n",
    "    print(\"cuDNN benchmark 模式已启用\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "id": "FqZSY8dujSo8",
    "papermill": {
     "duration": 0.004411,
     "end_time": "2025-04-13T01:53:56.494490",
     "exception": false,
     "start_time": "2025-04-13T01:53:56.490079",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 读取和准备数据\n",
    "从train_classification_label.xlsx读取标签数据，并组织预处理后的图像数据路径。标签包括：0（健康）、1（建议观察）、2（建议手术）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "id": "zkqTp50SjSo9",
    "outputId": "94d0a742-5487-4279-f124-ecd67b27bd80",
    "papermill": {
     "duration": 0.023889,
     "end_time": "2025-04-13T01:53:56.522681",
     "exception": false,
     "start_time": "2025-04-13T01:53:56.498792",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import sys\n",
    "\n",
    "# 如果在云端上运行，从 Google Drive 读取数据\n",
    "if 'google.colab' in sys.modules or os.path.exists(\"/kaggle/working\"):\n",
    "\n",
    "    if 'google.colab' in sys.modules:\n",
    "        print('在 Google Colab 环境中运行')\n",
    "        image_dir = os.path.join(colab_extract_path,\"train\")\n",
    "        label_file = os.path.join(image_dir,\"train_classification_label.xlsx\")\n",
    "        zip_path = colab_zip_path\n",
    "        extract_path = colab_extract_path\n",
    "\n",
    "        # Mount Google Drive\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "    else:\n",
    "        print('在 Kaggle 环境中运行')\n",
    "        # Kaggle 环境下的路径设置\n",
    "        # image_dir = os.path.join(kaggle_extract_path,\"train\")\n",
    "        # label_file = os.path.join(image_dir,\"train_classification_label.xlsx\")\n",
    "        # zip_path = kaggle_zip_path\n",
    "        # extract_path = kaggle_extract_path\n",
    "\n",
    "        # Google Drive 有每日下载次数限制，可能会导致下载失败\n",
    "        # if not os.path.exists(zip_path):\n",
    "        #     from kaggle_secrets import UserSecretsClient\n",
    "        #     user_secrets = UserSecretsClient()\n",
    "        #     !gdown --id {user_secrets.get_secret(\"train_zip_downloadurl\")}\n",
    "        image_dir = os.path.join(kaggle_extract_path,\"train\")\n",
    "        label_file = os.path.join(image_dir,\"train_classification_label.xlsx\")\n",
    "        val_image_dir = os.path.join(kaggle_val_path,\"val_img\")\n",
    "\n",
    "    if not os.path.exists(label_file):\n",
    "        # 解压数据\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_path)    \n",
    "else:\n",
    "    print(f'不在云端环境中运行,使用本地数据路径{image_dir}')\n",
    "label_file = os.path.join(image_dir,\"train_classification_label.xlsx\")\n",
    "\n",
    "# 自定义数据集类，用于读取图像和标签\n",
    "class PterygiumDataset(Dataset):\n",
    "    def __init__(self, label_file, image_dir, transform=None):\n",
    "        \"\"\"\n",
    "        初始化数据集\n",
    "        :param label_file: 包含图像标签的Excel文件路径\n",
    "        :param image_dir: 图像文件夹路径\n",
    "        :param transform: 图像变换操作\n",
    "        \"\"\"\n",
    "        self.labels_df = pd.read_excel(label_file)\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        获取指定索引的图像和标签\n",
    "        :param idx: 索引\n",
    "        :return: 图像张量和对应标签\n",
    "        \"\"\"\n",
    "        row = self.labels_df.iloc[idx]\n",
    "        image_name = row['Image']\n",
    "        label = row['Pterygium']\n",
    "        image_folder = f\"{int(image_name):04d}\"\n",
    "        image_path = os.path.join(self.image_dir, image_folder, f\"{image_folder}.png\")\n",
    "\n",
    "        # 加载图像\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        # 应用图像变换\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "papermill": {
     "duration": 0.004176,
     "end_time": "2025-04-13T01:53:56.531483",
     "exception": false,
     "start_time": "2025-04-13T01:53:56.527307",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 数据 Resize\n",
    "这一步骤是将图像调整为224x224的大小，以适应模型输入要求。\n",
    "只在Linux运行时使用，因为windows仅用与测试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "papermill": {
     "duration": 428.90724,
     "end_time": "2025-04-13T02:01:05.443030",
     "exception": false,
     "start_time": "2025-04-13T01:53:56.535790",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import to_pil_image\n",
    "target_size = (224, 224) # 目标尺寸\n",
    "output_format = \"PNG\" # 输出格式\n",
    "\n",
    "# --- Transformation Definition ---\n",
    "# We will perform Resize on GPU. ToTensor conversion happens before moving to GPU.\n",
    "# Normalization will be done *online* during training dataloading, not offline.\n",
    "resize_transform = transforms.Resize(target_size, interpolation=transforms.InterpolationMode.BILINEAR, antialias=True)\n",
    "# BILINEAR is a good default. antialias=True is recommended for downsampling.\n",
    "\n",
    "# --- Processing Function ---\n",
    "def resize_and_save_image(img_info, base_input_dir, base_output_dir, transform, device):\n",
    "    \"\"\"\n",
    "    Reads an image, resizes it (potentially on GPU), and saves it.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        image_name = img_info['Image']\n",
    "        image_name = f\"{int(image_name):04d}\"\n",
    "        if os.path.exists(os.path.join(base_input_dir, f\"{image_name}.png\")):\n",
    "            # 验证集图像路径\n",
    "            input_path = os.path.join(base_input_dir, f\"{image_name}.png\")\n",
    "            os.makedirs(base_output_dir, exist_ok=True)\n",
    "            output_path = os.path.join(base_output_dir, f\"{image_name}.{output_format.lower()}\")\n",
    "        else:\n",
    "            # 训练集图像路径\n",
    "            input_path = os.path.join(base_input_dir, image_name, f\"{image_name}.png\")\n",
    "            # Create corresponding output subdirectory if it doesn't exist\n",
    "            output_folder_path = os.path.join(base_output_dir, image_name)\n",
    "            os.makedirs(output_folder_path, exist_ok=True)\n",
    "            output_path = os.path.join(output_folder_path, f\"{image_name}.{output_format.lower()}\")\n",
    "\n",
    "        # 1. Read image using PIL (CPU)\n",
    "        img_pil = Image.open(input_path).convert(\"RGB\")\n",
    "\n",
    "        # 2. Convert PIL image to Tensor (CPU, scales to [0, 1])\n",
    "        img_tensor_cpu = transforms.functional.to_tensor(img_pil) # Output: CxHxW\n",
    "\n",
    "        # 3. Move tensor to GPU (if available)\n",
    "        img_tensor_gpu = img_tensor_cpu.to(device)\n",
    "\n",
    "        # 4. Apply Resize transform (GPU)\n",
    "        resized_tensor_gpu = transform(img_tensor_gpu)\n",
    "\n",
    "        # 5. Move resized tensor back to CPU\n",
    "        resized_tensor_cpu = resized_tensor_gpu.cpu()\n",
    "\n",
    "        # 6. Convert tensor back to PIL Image (CPU)\n",
    "        # to_pil_image expects CxHxW tensor in [0, 1] range\n",
    "        resized_img_pil = to_pil_image(resized_tensor_cpu)\n",
    "\n",
    "        # 7. Save the resized PIL image (CPU)\n",
    "        resized_img_pil.save(output_path, format=output_format)\n",
    "        \n",
    "        return True # Indicate success\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"错误: 文件未找到 {input_path}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"错误处理图像 {input_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "if not platform.system() == \"Windows\":\n",
    "    if 'google.colab' in sys.modules:\n",
    "        original_image_dir = os.path.join(colab_extract_path,\"train\")\n",
    "        output_dir = os.path.join(colab_extract_path,\"train_resized\")\n",
    "    elif os.path.exists(\"/kaggle/working\"):\n",
    "        original_image_dir = os.path.join(kaggle_extract_path,\"train\")\n",
    "        output_dir = os.path.join(kaggle_temp_path,\"train_resized\")\n",
    "    else:\n",
    "        print(\"错误: 无法识别的环境\")\n",
    "        exit(1)\n",
    "    image_dir = output_dir\n",
    "\n",
    "    print(f\"输入目录: {original_image_dir}\")\n",
    "    print(f\"输出目录: {output_dir}\")\n",
    "    print(f\"目标尺寸: {target_size}\")\n",
    "\n",
    "    # Create the main output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    if os.listdir(output_dir):\n",
    "        print(\"检测到已存在的resize数据，跳过resize步骤\")\n",
    "    else:\n",
    "        # Read label file to know which images to process\n",
    "        try:\n",
    "            labels_df = pd.read_excel(label_file)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"错误: 标签文件未找到 {label_file}\")\n",
    "            sys.exit(1) # Exit if label file is missing\n",
    "\n",
    "        success_count = 0\n",
    "        error_count = 0\n",
    "\n",
    "        # Iterate through images listed in the label file\n",
    "        for index, row in tqdm(labels_df.iterrows(), total=len(labels_df), desc=\"Resizing Images\"):\n",
    "            if resize_and_save_image(row, original_image_dir, output_dir, resize_transform, device):\n",
    "                success_count += 1\n",
    "            else:\n",
    "                error_count += 1\n",
    "\n",
    "        print(f\"\\n处理完成!\")\n",
    "        print(f\"成功处理图像数: {success_count}\")\n",
    "        print(f\"处理失败图像数: {error_count}\")\n",
    "        print(f\"处理后的图像保存在: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "id": "okNIq-rfjSo-",
    "papermill": {
     "duration": 0.004356,
     "end_time": "2025-04-13T02:01:05.452384",
     "exception": false,
     "start_time": "2025-04-13T02:01:05.448028",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 创建数据加载器\n",
    "使用PyTorch的Dataset和DataLoader类创建数据集和加载器，包括数据增强和训练/验证集的划分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "id": "11JDwC__jSo-",
    "papermill": {
     "duration": 1.465971,
     "end_time": "2025-04-13T02:01:06.922765",
     "exception": false,
     "start_time": "2025-04-13T02:01:05.456794",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 数据变换\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)), # 先放大一点\n",
    "    transforms.RandomCrop((224, 224)), # 随机裁剪回目标尺寸\n",
    "    transforms.RandomHorizontalFlip(p=0.5), # 随机水平翻转\n",
    "    transforms.RandomRotation(degrees=15), # 随机旋转\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1), # 随机颜色抖动\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 定义验证集/测试集的变换 (无需数据增强)\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 划分训练集和验证集，并创建对应的数据加载器\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 读取标签文件\n",
    "labels_df = pd.read_excel(label_file)\n",
    "\n",
    "# 按照8:2的比例划分训练集和验证集\n",
    "train_df, val_df = train_test_split(labels_df, test_size=0.2, random_state=42, stratify=labels_df['Pterygium'])\n",
    "\n",
    "# 保存划分后的数据集到文件\n",
    "train_label_file = os.path.join(image_dir, \"train_classification_label_train.xlsx\")\n",
    "val_label_file = os.path.join(image_dir, \"train_classification_label_val.xlsx\")\n",
    "if os.path.exists(\"/kaggle/working\"):\n",
    "    train_label_file = os.path.join(kaggle_temp_path, \"train_classification_label_train.xlsx\")\n",
    "    val_label_file = os.path.join(kaggle_temp_path, \"train_classification_label_val.xlsx\")\n",
    "train_df.to_excel(train_label_file, index=False)\n",
    "val_df.to_excel(val_label_file, index=False)\n",
    "\n",
    "# 创建训练集和验证集的数据集对象 (使用不同的 transform)\n",
    "train_dataset = PterygiumDataset(label_file=train_label_file, image_dir=image_dir, transform=train_transform) # 使用训练变换\n",
    "val_dataset = PterygiumDataset(label_file=val_label_file, image_dir=image_dir, transform=val_transform) # 使用验证变换\n",
    "\n",
    "# 创建训练集和验证集的数据加载器\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                        batch_size=64,\n",
    "                        shuffle=True,\n",
    "                        num_workers=num_workers,\n",
    "                        prefetch_factor=2 if platform.system() == \"Windows\" else 10,\n",
    "                        pin_memory=False if platform.system() == \"Windows\" else True)\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                        batch_size=64,\n",
    "                        shuffle=False,\n",
    "                        num_workers=num_workers,\n",
    "                        prefetch_factor=2 if platform.system() == \"Windows\" else 10,\n",
    "                        pin_memory=False if platform.system() == \"Windows\" else True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {
    "id": "eGn3VBzHjSo_",
    "papermill": {
     "duration": 0.004468,
     "end_time": "2025-04-13T02:01:06.932847",
     "exception": false,
     "start_time": "2025-04-13T02:01:06.928379",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 构建 ResNet 模型\n",
    "使用PyTorch的预训练ResNet18模型，修改最后的全连接层以适应3个类别的分类任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "id": "7wYl9yfzjSpA",
    "outputId": "202683a8-91b6-4ab9-b987-aa1f28aa768b",
    "papermill": {
     "duration": 0.692259,
     "end_time": "2025-04-13T02:01:07.629659",
     "exception": false,
     "start_time": "2025-04-13T02:01:06.937400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 构建 ResNet18 模型\n",
    "from torchvision.models import ResNet18_Weights\n",
    "class ResNet18Classifier(nn.Module):\n",
    "    def __init__(self, num_classes=3, dropout_rate=0.5):\n",
    "        super(ResNet18Classifier, self).__init__()\n",
    "        # 加载预训练的 ResNet18 模型\n",
    "        self.resnet18 = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        # 替换最后的全连接层以适应3个类别的分类任务\n",
    "        in_features = self.resnet18.fc.in_features\n",
    "        self.resnet18.fc = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_rate), # 添加 Dropout 层\n",
    "            nn.Linear(in_features, num_classes) # 添加新的全连接层\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet18(x)\n",
    "    \n",
    "# 构建 ResNet34 模型\n",
    "from torchvision.models import ResNet34_Weights\n",
    "class ResNet34Classifier(nn.Module):\n",
    "    def __init__(self, num_classes=3, dropout_rate=0.5):\n",
    "        super(ResNet34Classifier, self).__init__()\n",
    "        self.resnet34 = models.resnet34(weights=ResNet34_Weights.IMAGENET1K_V1)\n",
    "        in_features = self.resnet34.fc.in_features\n",
    "        self.resnet34.fc = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_rate), # 添加 Dropout 层\n",
    "            nn.Linear(in_features, num_classes) # 添加新的全连接层\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet34(x)\n",
    "    \n",
    "# 构建 ResNet34 模型\n",
    "from torchvision.models import ResNet50_Weights\n",
    "class ResNet50Classifier(nn.Module):\n",
    "    def __init__(self, num_classes=3, dropout_rate=0.5):\n",
    "        super(ResNet50Classifier, self).__init__()\n",
    "        self.resnet50 = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "        in_features = self.resnet50.fc.in_features\n",
    "        self.resnet50.fc = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_rate), # 添加 Dropout 层\n",
    "            nn.Linear(in_features, num_classes) # 添加新的全连接层\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet50(x)\n",
    "    \n",
    "# 定义模型\n",
    "model = ResNet18Classifier(num_classes=3).to(device)\n",
    "\n",
    "# 将模型移动到 GPU（如果可用）\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {
    "id": "dCYYlSWDjSpB",
    "papermill": {
     "duration": 0.005309,
     "end_time": "2025-04-13T02:01:07.640990",
     "exception": false,
     "start_time": "2025-04-13T02:01:07.635681",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 定义带正则化项的损失函数\n",
    "实现一个包含正则化项的损失函数，使用交叉熵损失作为基础，并添加特定的正则化项来抑制高光问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "id": "9tVvDaYLjSpB",
    "papermill": {
     "duration": 0.012467,
     "end_time": "2025-04-13T02:01:07.658402",
     "exception": false,
     "start_time": "2025-04-13T02:01:07.645935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定义损失函数，包含正则化项以抑制高光问题\n",
    "class HighlightRegularizedLoss(nn.Module):\n",
    "    def __init__(self, base_loss_fn, lambda_reg=0.01):\n",
    "        super(HighlightRegularizedLoss, self).__init__()\n",
    "        self.base_loss_fn = base_loss_fn\n",
    "        self.lambda_reg = lambda_reg\n",
    "\n",
    "    def forward(self, outputs, targets, inputs):\n",
    "        # 基础损失（交叉熵损失）\n",
    "        base_loss = self.base_loss_fn(outputs, targets)\n",
    "\n",
    "        # 正则化项：抑制高光问题（假设高光区域的像素值接近1）\n",
    "        highlight_penalty = torch.mean(torch.clamp(inputs - 0.9, min=0) ** 2)\n",
    "\n",
    "        # 总损失\n",
    "        total_loss = base_loss #+ self.lambda_reg * highlight_penalty\n",
    "        return total_loss\n",
    "\n",
    "# 定义基础损失函数（交叉熵损失）\n",
    "base_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# 定义包含正则化项的损失函数\n",
    "criterion = HighlightRegularizedLoss(base_loss_fn=base_loss_fn, lambda_reg=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {
    "id": "SjMzFIyejSpC",
    "papermill": {
     "duration": 0.004709,
     "end_time": "2025-04-13T02:01:07.668189",
     "exception": false,
     "start_time": "2025-04-13T02:01:07.663480",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 配置优化器和训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "id": "0NUONUTJjSpC",
    "papermill": {
     "duration": 0.011721,
     "end_time": "2025-04-13T02:01:07.685038",
     "exception": false,
     "start_time": "2025-04-13T02:01:07.673317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 配置优化器和学习率调度器\n",
    "# 在Adam优化器中添加 weight_decay 参数实现L2正则化\n",
    "optimizer = optim.AdamW(model.parameters(), lr=7e-4, weight_decay=1e-4)\n",
    "\n",
    "# 定义学习率调度器，采用余弦退火调度策略\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=25, eta_min=1e-6)\n",
    "\n",
    "# 设置其他训练参数\n",
    "num_epochs = 25  # 训练的总轮数\n",
    "log_interval = 10  # 每隔多少个批次打印一次日志"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {
    "id": "0dKuzXaHjSpD",
    "papermill": {
     "duration": 0.005112,
     "end_time": "2025-04-13T02:01:07.695500",
     "exception": false,
     "start_time": "2025-04-13T02:01:07.690388",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 训练模型\n",
    "实现训练循环，包括前向传播、损失计算、反向传播和参数更新，并记录训练过程中的指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "papermill": {
     "duration": 0.014088,
     "end_time": "2025-04-13T02:01:07.714614",
     "exception": false,
     "start_time": "2025-04-13T02:01:07.700526",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定义早停类\n",
    "from copy import deepcopy\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, min_delta=0.0, mode='max'):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta # 允许的最小提升量\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.mode = mode\n",
    "        self.best_model_weights = None\n",
    "\n",
    "        # 根据模式确定比较操作\n",
    "        if self.mode == 'min':\n",
    "            self.delta_sign = -1 # 对于最小值模式，分数需要减少 delta\n",
    "        else: # mode == 'max'\n",
    "            self.delta_sign = 1 # 对于最大值模式，分数需要增加 delta\n",
    "\n",
    "    def __call__(self, val_score, model):\n",
    "        score = val_score # 直接使用验证分数\n",
    "\n",
    "        if self.best_score is None:\n",
    "            # 第一次调用，初始化最佳分数并保存权重\n",
    "            self.best_score = score\n",
    "            self.best_model_weights = deepcopy(model.state_dict())\n",
    "            tqdm.write(f\"EarlyStopping: Initialized best score to {self.best_score:.4f}\")\n",
    "        # 检查是否有足够的提升\n",
    "        elif (score * self.delta_sign) > (self.best_score * self.delta_sign) + self.min_delta:\n",
    "            # 有足够的提升\n",
    "            self.best_score = score\n",
    "            self.best_model_weights = deepcopy(model.state_dict())\n",
    "            self.counter = 0\n",
    "            tqdm.write(f\"EarlyStopping: Improvement found. Best score updated to {self.best_score:.4f}. Counter reset.\")\n",
    "        else:\n",
    "            # 没有足够的提升\n",
    "            self.counter += 1\n",
    "            tqdm.write(f'EarlyStopping counter: {self.counter} out of {self.patience}. Best score remains {self.best_score:.4f}.')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                tqdm.write(\"EarlyStopping: Patience reached.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "id": "9wq254XdjSpD",
    "papermill": {
     "duration": 49.831481,
     "end_time": "2025-04-13T02:01:57.551548",
     "exception": false,
     "start_time": "2025-04-13T02:01:07.720067",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import time\n",
    "\n",
    "def train_validate_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device, run_identifier=\"Run\"):\n",
    "    \"\"\"\n",
    "    训练并验证模型一个完整的周期，支持早停。\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): 要训练的模型实例。\n",
    "        train_loader (DataLoader): 训练数据加载器。\n",
    "        val_loader (DataLoader): 验证数据加载器。\n",
    "        criterion (nn.Module): 损失函数。\n",
    "        optimizer (Optimizer): 优化器。\n",
    "        scheduler (LRScheduler): 学习率调度器。\n",
    "        num_epochs (int): 最大训练轮数。\n",
    "        device (torch.device): 计算设备 ('cuda' or 'cpu')。\n",
    "        run_identifier (str): 用于日志输出的运行标识符。\n",
    "\n",
    "    Returns:\n",
    "        tuple: 包含以下元素的元组:\n",
    "            - float: 最佳验证准确率 (%)。\n",
    "            - float: 最佳验证Macro F1分数。\n",
    "            - list: 每个epoch的验证准确率历史。\n",
    "            - dict: 最佳模型的state_dict。\n",
    "    \"\"\"\n",
    "    start_time_run = time.time()\n",
    "    print(f\"\\n--- {run_identifier}: 开始训练 ---\")\n",
    "\n",
    "    # 初始化此运行所需的状态对象\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "    early_stopping = EarlyStopping(patience=7, mode='max') # 与之前相同的早停设置\n",
    "    val_accuracy_history = [] # 存储当前运行的验证准确率历史\n",
    "    best_model_state_dict = None # 存储最佳模型的状态\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # --- 训练阶段 ---\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        train_loader_tqdm = tqdm(train_loader, desc=f'{run_identifier} Epoch {epoch+1}/{num_epochs}', leave=False)\n",
    "\n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader_tqdm):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                outputs = model(inputs)\n",
    "                # 确保 criterion 也接收 inputs 如果它需要的话 (比如你的 HighlightRegularizedLoss)\n",
    "                if isinstance(criterion, HighlightRegularizedLoss):\n",
    "                    loss = criterion(outputs, targets, inputs)\n",
    "                else:\n",
    "                    loss = criterion(outputs, targets) # 标准损失函数\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            train_loader_tqdm.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{100. * correct / total:.2f}%',\n",
    "                'lr': f'{current_lr:.1e}'\n",
    "            })\n",
    "\n",
    "        scheduler.step() # 每个epoch后更新学习率\n",
    "\n",
    "        # --- 验证阶段 ---\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                with torch.amp.autocast('cuda'):\n",
    "                    outputs = model(inputs)\n",
    "                    if isinstance(criterion, HighlightRegularizedLoss):\n",
    "                        loss = criterion(outputs, targets, inputs)\n",
    "                    else:\n",
    "                        loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += targets.size(0)\n",
    "                val_correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        val_accuracy = 100. * val_correct / val_total\n",
    "        val_accuracy_history.append(val_accuracy)\n",
    "\n",
    "        tqdm.write(f\"{run_identifier} Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss / len(train_loader):.4f}, \"\n",
    "                f\"Train Acc: {100. * correct / total:.2f}%, Val Loss: {val_loss / len(val_loader):.4f}, \"\n",
    "                f\"Val Acc: {val_accuracy:.2f}%\")\n",
    "\n",
    "        # --- 早停检查 ---\n",
    "        # 注意: EarlyStopping现在在内部使用tqdm.write\n",
    "        early_stopping(val_accuracy, model)\n",
    "        if early_stopping.early_stop:\n",
    "            tqdm.write(f\"{run_identifier}: EarlyStopping triggered at epoch {epoch + 1}.\")\n",
    "            best_model_state_dict = early_stopping.best_model_weights # 获取最佳权重\n",
    "            break # 提前结束训练\n",
    "\n",
    "    # 如果训练正常完成（未早停），也要保存最后的最佳权重\n",
    "    if not early_stopping.early_stop:\n",
    "        tqdm.write(f\"{run_identifier}: Training finished after {num_epochs} epochs.\")\n",
    "        best_model_state_dict = early_stopping.best_model_weights # 获取最后记录的最佳权重\n",
    "\n",
    "    # --- 使用最佳模型进行最终评估 ---\n",
    "    if best_model_state_dict:\n",
    "        model.load_state_dict(best_model_state_dict) # 加载最佳权重\n",
    "        model.eval()\n",
    "        final_all_targets = []\n",
    "        final_all_predictions = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = outputs.max(1)\n",
    "                final_all_targets.extend(targets.cpu().numpy())\n",
    "                final_all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "        final_accuracy = accuracy_score(final_all_targets, final_all_predictions) * 100\n",
    "        final_macro_f1 = f1_score(final_all_targets, final_all_predictions, average='macro')\n",
    "    else:\n",
    "        # 如果由于某种原因没有最佳权重（例如训练在第一个epoch就停止且未改进）\n",
    "        final_accuracy = 0.0\n",
    "        final_macro_f1 = 0.0\n",
    "        tqdm.write(f\"{run_identifier}: Warning - Could not obtain best model weights.\")\n",
    "\n",
    "\n",
    "    end_time_run = time.time()\n",
    "    print(f\"--- {run_identifier}: 训练完成 ---\")\n",
    "    print(f\"最终验证准确率: {final_accuracy:.4f}%\")\n",
    "    print(f\"最终验证Macro F1: {final_macro_f1:.4f}\")\n",
    "    print(f\"本次运行耗时: {end_time_run - start_time_run:.2f} 秒\")\n",
    "\n",
    "    return final_accuracy, final_macro_f1, val_accuracy_history, best_model_state_dict\n",
    "\n",
    "train_validate_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        num_epochs=num_epochs, # 使用之前定义的 num_epochs\n",
    "        device=device,\n",
    "        run_identifier=None\n",
    "    );\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {
    "id": "peWRUqyKjSpE",
    "papermill": {
     "duration": 0.006745,
     "end_time": "2025-04-13T02:01:57.566762",
     "exception": false,
     "start_time": "2025-04-13T02:01:57.560017",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 评估模型性能\n",
    "在验证集上评估模型性能，计算准确率、混淆矩阵、F1分数等指标，并可视化结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "id": "xzQdrZkTjSpE",
    "papermill": {
     "duration": 1.208724,
     "end_time": "2025-04-13T02:01:58.782451",
     "exception": false,
     "start_time": "2025-04-13T02:01:57.573727",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score\n",
    "import seaborn as sns\n",
    "\n",
    "# 评估模型性能\n",
    "model.eval()  # 设置模型为评估模式\n",
    "all_targets = []\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in val_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # 前向传播\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # 获取预测结果\n",
    "        _, predicted = outputs.max(1)\n",
    "        all_targets.extend(targets.cpu().numpy())\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "# 计算评估指标\n",
    "accuracy = accuracy_score(all_targets, all_predictions)\n",
    "macro_precision = precision_score(all_targets, all_predictions, average='macro')\n",
    "macro_f1 = f1_score(all_targets, all_predictions, average='macro')\n",
    "conf_matrix = confusion_matrix(all_targets, all_predictions)\n",
    "\n",
    "print(f\"验证集准确率: {accuracy:.4f}\")\n",
    "print(f\"验证集Macro Precision: {macro_precision:.4f}\")\n",
    "print(f\"验证集Macro F1分数: {macro_f1:.4f}\")\n",
    "\n",
    "# 可视化混淆矩阵\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"健康\", \"建议观察\", \"建议手术\"],\n",
    "            yticklabels=[\"健康\", \"建议观察\", \"建议手术\"])\n",
    "plt.xlabel(\"预测标签\")\n",
    "plt.ylabel(\"真实标签\")\n",
    "plt.title(\"混淆矩阵\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {
    "papermill": {
     "duration": 0.007431,
     "end_time": "2025-04-13T02:01:58.797761",
     "exception": false,
     "start_time": "2025-04-13T02:01:58.790330",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 模型保存和加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "papermill": {
     "duration": 0.083802,
     "end_time": "2025-04-13T02:01:58.888785",
     "exception": false,
     "start_time": "2025-04-13T02:01:58.804983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 保存模型参数\n",
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"模型参数已保存到 {path}\")\n",
    "\n",
    "# 加载模型参数\n",
    "def load_model(model, path, device):\n",
    "    model.load_state_dict(torch.load(path, map_location=device, weights_only=True))\n",
    "    model = model.to(device)\n",
    "    print(f\"模型参数已从 {path} 加载\")\n",
    "    return model\n",
    "\n",
    "# 保存训练好的模型\n",
    "model_save_path = \"./resnet18base_pterygium_classifier_base.pth\"\n",
    "save_model(model, model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {
    "id": "y6VojVHqjSpF",
    "papermill": {
     "duration": 0.007359,
     "end_time": "2025-04-13T02:01:58.904324",
     "exception": false,
     "start_time": "2025-04-13T02:01:58.896965",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 模型测试和预测\n",
    "使用训练好的模型对新图像进行预测，并展示几个预测示例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "id": "W3PhSqxEjSpF",
    "papermill": {
     "duration": 0.318951,
     "end_time": "2025-04-13T02:01:59.230622",
     "exception": false,
     "start_time": "2025-04-13T02:01:58.911671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 模型测试和预测\n",
    "def predict_image(model, image_path, transform, device):\n",
    "    \"\"\"\n",
    "    使用训练好的模型对单张图像进行预测\n",
    "    :param model: 训练好的模型\n",
    "    :param image_path: 图像路径\n",
    "    :param transform: 图像预处理变换\n",
    "    :param device: 设备（CPU 或 GPU）\n",
    "    :return: 预测类别\n",
    "    \"\"\"\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "    image = Image.open(image_path).convert(\"RGB\")  # 加载图像并转换为RGB\n",
    "    image = transform(image).unsqueeze(0).to(device)  # 应用预处理并添加批次维度\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)  # 前向传播\n",
    "        _, predicted = outputs.max(1)  # 获取预测类别\n",
    "    return predicted.item()\n",
    "\n",
    "def test_model_on_val(model, device, input_dir, temp_dir, transform):\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    image_paths = glob.glob(os.path.join(input_dir, \"*.png\"))\n",
    "    # 删除临时目录中的旧文件\n",
    "    for img_path in glob.glob(os.path.join(temp_dir, \"*.png\")):\n",
    "        os.remove(img_path)\n",
    "    results = []\n",
    "    \n",
    "    for img_path in tqdm(image_paths, desc=\"Processing images\", leave=True):\n",
    "        base_name = os.path.splitext(os.path.basename(img_path))[0]  # ...获取文件名（不带扩展）...\n",
    "        output_path = os.path.join(temp_dir, f\"{base_name}.{output_format.lower()}\")\n",
    "        if not os.path.exists(output_path):\n",
    "            try:\n",
    "                if resize_and_save_image({\"Image\": base_name}, input_dir, temp_dir, resize_transform, device):\n",
    "                    predicted_class = predict_image(model, output_path, transform, device)\n",
    "                    results.append({\"Image\": base_name, \"Pterygium\": predicted_class})\n",
    "                else:\n",
    "                    tqdm.write(f\"Resize失败: {img_path}\")\n",
    "            except Exception as e:\n",
    "                tqdm.write(f\"处理图像 {img_path} 时出错: {e}\")\n",
    "    \n",
    "    results.sort(key=lambda x: int(x[\"Image\"]))\n",
    "    df = pd.DataFrame(results, columns=[\"Image\", \"Pterygium\"])\n",
    "    result_path = os.path.join(kaggle_temp_path, \"Classification_Results.xlsx\")\n",
    "    df.to_excel(result_path, index=False)\n",
    "    print(f\"分类结果已保存到 {result_path}\")\n",
    "    print(f'删除验证集缓存数据 {temp_dir}')\n",
    "    for img_path in glob.glob(os.path.join(temp_dir, \"*.png\")):\n",
    "        os.remove(img_path)\n",
    "\n",
    "# 加载保存的模型并进行推理\n",
    "loaded_model = ResNet18Classifier(num_classes=3)\n",
    "loaded_model = load_model(loaded_model, model_save_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "papermill": {
     "duration": 0.057612,
     "end_time": "2025-04-13T02:01:59.296113",
     "exception": false,
     "start_time": "2025-04-13T02:01:59.238501",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # 单文件推理\n",
    "    test_image_path = \"./test_images/sample_image.png\"  # 替换为实际测试图像路径\n",
    "    predicted_class = predict_image(loaded_model, test_image_path, val_transform, device)\n",
    "    class_names = [\"健康\", \"建议观察\", \"建议手术\"]\n",
    "    print(f\"加载模型后预测结果: 图像 {os.path.basename(test_image_path)}, 预测类别: {class_names[predicted_class]}\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "papermill": {
     "duration": 140.716871,
     "end_time": "2025-04-13T02:04:20.020553",
     "exception": false,
     "start_time": "2025-04-13T02:01:59.303682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 验证集推理\n",
    "val_temp_dir = os.path.join(kaggle_temp_path, \"val_resized\")\n",
    "test_model_on_val(loaded_model, device, val_image_dir, val_temp_dir, val_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {
    "papermill": {
     "duration": 0.007368,
     "end_time": "2025-04-13T02:04:20.035996",
     "exception": false,
     "start_time": "2025-04-13T02:04:20.028628",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 基线模型波动范围检测\n",
    "为了理解模型性能的自然波动范围，我们将使用完全相同的设置重复训练过程10次。\n",
    "每次运行都会重新初始化模型权重、优化器和早停策略。\n",
    "我们将记录每次运行最终的验证集准确率和Macro F1分数，并计算平均值和标准差。\n",
    "同时，我们也会保存每次运行的学习曲线（验证准确率随epoch的变化）并进行可视化。\n",
    "\n",
    "**警告:** 这将花费大约10倍的单次训练时间！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "papermill": {
     "duration": 295.540911,
     "end_time": "2025-04-13T02:09:15.584272",
     "exception": false,
     "start_time": "2025-04-13T02:04:20.043361",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 实验参数 ---\n",
    "num_runs = 5  # 重复运行的次数\n",
    "base_seed = 42 # 基础随机种子，每次运行会递增\n",
    "\n",
    "# --- 存储结果 ---\n",
    "all_final_val_accuracies = []\n",
    "all_final_macro_f1_scores = []\n",
    "all_learning_curves = [] # 存储每个run的验证准确率历史\n",
    "# best_model_state_dicts = [] # 如果需要保存每次运行的最佳模型权重\n",
    "\n",
    "# --- 定义损失函数 (可以在循环外定义，因为无状态) ---\n",
    "base_loss_fn = nn.CrossEntropyLoss()\n",
    "# 注意：确保 HighlightRegularizedLoss 类已经定义\n",
    "criterion = HighlightRegularizedLoss(base_loss_fn=base_loss_fn, lambda_reg=0.01) # 再次注意：正则项仍未启用\n",
    "\n",
    "# --- 开始多次运行循环 ---\n",
    "print(f\"开始进行 {num_runs} 次独立训练运行以评估波动性...\")\n",
    "\n",
    "for run in range(num_runs):\n",
    "    run_id_str = f\"Run {run + 1}/{num_runs}\"\n",
    "    print(f\"\\n--- 开始 {run_id_str} ---\")\n",
    "\n",
    "    # --- 1. 设置随机种子 ---\n",
    "    current_seed = base_seed + run\n",
    "    torch.manual_seed(current_seed)\n",
    "    np.random.seed(current_seed)\n",
    "    random.seed(current_seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(current_seed)\n",
    "        # 下面两行可确保重复性，但是降低性能\n",
    "        # torch.backends.cudnn.deterministic = True\n",
    "        # torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # --- 2. 创建此运行所需的模型、优化器和调度器实例 ---\n",
    "    # **每次循环都必须创建新的实例**\n",
    "    model = ResNet18Classifier(num_classes=3).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=7e-4, weight_decay=1e-4) # 使用基线超参数\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=25, eta_min=1e-6) # 使用基线超参数\n",
    "    #scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=3)\n",
    "    \n",
    "    # --- 3. 调用训练函数 ---\n",
    "    # train_loader 和 val_loader 是之前已经定义好的\n",
    "    final_acc, final_f1, history, best_state = train_validate_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        num_epochs=num_epochs,\n",
    "        device=device,\n",
    "        run_identifier=run_id_str\n",
    "    )\n",
    "\n",
    "    # --- 4. 存储结果 ---\n",
    "    all_final_val_accuracies.append(final_acc)\n",
    "    all_final_macro_f1_scores.append(final_f1)\n",
    "    all_learning_curves.append(history)\n",
    "    # if best_state: # 如果需要保存权重\n",
    "    #     best_model_state_dicts.append(best_state)\n",
    "\n",
    "# --- 5. 多次运行结束后，进行分析和可视化 ---\n",
    "print(\"\\n--- 多次运行结果分析 ---\")\n",
    "\n",
    "# 计算平均值和标准差\n",
    "mean_accuracy = np.mean(all_final_val_accuracies)\n",
    "std_accuracy = np.std(all_final_val_accuracies)\n",
    "mean_f1 = np.mean(all_final_macro_f1_scores)\n",
    "std_f1 = np.std(all_final_macro_f1_scores)\n",
    "\n",
    "print(f\"平均验证准确率: {mean_accuracy:.4f}%\")\n",
    "print(f\"验证准确率标准差: {std_accuracy:.4f}%\")\n",
    "print(f\"平均验证Macro F1: {mean_f1:.4f}\")\n",
    "print(f\"验证Macro F1标准差: {std_f1:.4f}\")\n",
    "\n",
    "print(\"\\n每次运行的最终验证准确率:\")\n",
    "for i, acc in enumerate(all_final_val_accuracies):\n",
    "    print(f\"  Run {i+1}: {acc:.4f}%\")\n",
    "\n",
    "print(\"\\n每次运行的最终验证Macro F1:\")\n",
    "for i, f1 in enumerate(all_final_macro_f1_scores):\n",
    "    print(f\"  Run {i+1}: {f1:.4f}\")\n",
    "\n",
    "# --- 6. 可视化学习曲线 (与之前相同) ---\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, curve in enumerate(all_learning_curves):\n",
    "    epochs_run = range(1, len(curve) + 1)\n",
    "    plt.plot(epochs_run, curve, label=f'Run {i+1}', alpha=0.7)\n",
    "\n",
    "plt.title('不同运行的验证准确率学习曲线')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('验证准确率 (%)')\n",
    "plt.legend(loc='lower right', ncol=2)\n",
    "plt.grid(True)\n",
    "# 动态调整Y轴范围，确保所有曲线可见\n",
    "min_y_val = np.min([min(c) for c in all_learning_curves if c]) if any(all_learning_curves) else 80 # 如果有数据则取最小值，否则默认80\n",
    "max_y_val = np.max([max(c) for c in all_learning_curves if c]) if any(all_learning_curves) else 100 # 如果有数据则取最大值，否则默认100\n",
    "plt.ylim(bottom=max(0, min_y_val - 2), top=min(100, max_y_val + 2)) # 确保在[0, 100]范围内，并留出边距\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7046642,
     "sourceId": 11272397,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 231745112,
     "sourceType": "kernelVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 937.871658,
   "end_time": "2025-04-13T02:09:18.610590",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-13T01:53:40.738932",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
